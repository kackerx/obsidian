## 查询语句
![[Pasted image 20230320164426.png|555]]
![[Pasted image 20230313141841.png|555]]
- 连接->分析->优化->执行

## 更新语句
![[Pasted image 20230313161245.png|555]]
- 执行器根据索引找到id=2, 判断是在内存中还是需要读盘
- 执行器把这个值进行更行后, 引擎将新值写入内存, 同时同步操作记录到redo
- <mark class="hltr-pink">此时redo处于prepare</mark>, 告知引擎自己随时可以提交
- 执行器生成这个操作的binlog, 写入磁盘
- 执行器调用引擎提交事务接口, 引擎更新<mark class="hltr-pink">redo状态为commit</mark>, 更新结束

## 二阶段提交, prepare和commit

- 写入redolog处于prepare和写binlog之间的阶段如果宕机, 那么这条没有commit, 数据是一致的
- 使用二阶段提交, 最终commit前的操作都是准备中, 最终提交之前失败都不会影响一致性
- 提交事务的时候, 一定是redo log写入磁盘, binlog写入磁盘, 完成redo log的事务commit标记, 最后后台的io线程随机把buffer pool的脏数据刷入磁盘
![[Pasted image 20230320175710.png|555]]
## 二阶段提交

```ad-tips
如果不用二阶段提交, 会发生什么? 

- 要么先写bin, bin写完崩溃了, bin有, 恢复或者从机同步时多了一条, 但是崩溃恢复后redo没有记录, 事务无效, 数据不一致
- 要么先写redo, redo写完崩溃, bin没有, 恢复或者从机同步少了一条, 崩溃恢复redo多这一条, 不一致
```

& 主要看<mark class="hltr-orange">写入bin前后</mark>的崩溃时机
- redolog处于prepare时崩溃(<mark class="hltr-pink">写bin前</mark>): 检查redo的记录然后去查相应的binlog, 没有这条就回滚, bin也不会同步到从库
- binlog提交后崩溃(<mark class="hltr-pink">写bin后</mark>): 检查bin的记录是否存在并完整, 对应的redo可以决定是提交还是回滚

![[Pasted image 20230405213122.png|555]]

---
:::::::::::::::::::::::::::::::::::::::: {.columns border=off col-count=2 largest-column=firs}

~~~ad-tips
title: 二阶段提交

- redo log影响主库数据(用于crash后本地恢复)
- bin log影响从库数据(同步从库)
- 两者不一致, 影响主从一致
- 如图, <mark class="hltr-pink">redo log</mark>`prepare`和`commit`, 中间穿插写入<mark class="hltr-cyan">binlog</mark>
	- 1, prepare: XID写入redo, redo对应事务状态为`prepare`, redo持久化磁盘(innodb_flush_log_at_trx_commit=1)
	- 2, commit: XID写入binlog, binlog持久化磁盘(sync_binlog=1), Innodb提交事务接口, redo状态为`commit`
~~~

```ad-bug
title: 异常情况

- 只有<mark class="hltr-blue">时刻A</mark>, 和<mark class="hltr-cyan">时刻B</mark>两种异常情况, 这两种情况下, 重启都会扫描redo log文件, 若处于`prepare`就用redo log的XID去binlog查是否存在
-  binlog中没有 ? 说明时刻A崩溃, 回滚 : 时刻B崩溃, 继续提交

```

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::columnbreak
:::

![[Pasted image 20230712120602.png|555]]

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
